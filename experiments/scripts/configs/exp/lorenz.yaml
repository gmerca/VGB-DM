name_exp: 'lorenz'

out_path: "./outputs/lorenz/"

algorithm: "dyn-fm" # ['dyn-fm','node']

seed: 3

dataset:
  data_path_tr: "./experiments/dataset/lorenz_attractor/train/tr_lorenz_data_seed_1_f99f234a781aeda45d3ec612da565d6b.pt"
  data_path_va: "./experiments/dataset/lorenz_attractor/val/val_lorenz_data_seed_2_1ec97522a1b023702f6b3e4ba4eba3b5.pt"

sampler:
  max_length: -1
  sampler_mode: "pairs-history" #['seq-pairs','trajectory', 'pairs-history']
  enc_len_episode: 30 # used in pairs-history
  n_pairs_per_traj: 20 # Thus: n_samples = batch_size * n_pairs_per_traj

enc_model:
  len_episode: 30 #
  state_dim: 3
  p_dim: 2 # 2
  z_dim: 8 # 8
  gru_hidden: 128  # 96
  p_prior_type: 'gaussian'
  p_prior_mean: [10.0, 2.7] # mean and 95 % CI for sigma, beta (tight bounds). 
  p_prior_std:  [1.2, 0.3] # [0.3, 0.06]

vf_model:
  dim_state: 3
  t_freq_dim:  10
  history_size: 4
  hidden_dims: [128,128,128,128]
  activation: SELU
  time_varying: False
  phys_model: True
  vf_phys: True
  interpolation: "linear" #['linear', 'lagrange']
  ode_method: "dopri5" #['euler',rk4','dopri5']
  val_ode_method: "dopri5"
  sigma: 0.0
  second_order: False
  

optimization:
  n_epochs: 2500
  batch_size: 512 #256
  vf_lr:  1e-3
  enc_lr: 1e-4
  vf_weight_decay: 5e-7
  enc_weight_decay: 5e-7
  use_kl_annealing: True       # Set to False to disable
  kl_anneal_steps: 1000 #5000        # Linear ramp over 5000 batches
  coeff_R: 0.0
  gamma_acc: 0.0
  beta_p: 0.1 # 1.0, 0.01
  beta_z: 0.1 # 1.0, 0.3
  alpha: 0.01
  beta: 0.0
  gamma: 0.0 # 1e-5
  phys_weight_scheduler: True
  phys_warming_steps: 250

training:
  log_val_step: 25 
  log_train_step: 20
  after_val_step: 250
  early_stopping: false
  max_time_tr: 120 # in minutes

max_patience_counter: 20

wandb_config_file: "experiments/scripts/configs/wandb/wandb_config.yaml"